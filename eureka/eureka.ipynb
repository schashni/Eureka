{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c490b41c-ff7d-4a07-ac6d-5f94e09bee4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T22:55:15.826449147Z",
     "start_time": "2024-03-08T22:55:15.825017881Z"
    }
   },
   "outputs": [],
   "source": [
    "# import isaacgym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01386340-a4de-44f4-a0f9-7c21a6fa7bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T23:12:24.154415540Z",
     "start_time": "2024-03-08T23:12:24.112360645Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5500b97-d96b-44cc-bd13-21e1498d4439",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T23:13:57.853302352Z",
     "start_time": "2024-03-08T23:13:56.888904728Z"
    }
   },
   "outputs": [],
   "source": [
    "import hydra\n",
    "import numpy as np \n",
    "import json\n",
    "import logging \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import openai\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time \n",
    "\n",
    "from utils.misc import * \n",
    "from utils.file_utils import find_files_with_substring, load_tensorboard_logs\n",
    "from utils.create_task import create_task\n",
    "from utils.extract_task_code import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a36d8df7-0e3e-4471-bd07-a53368efc2e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T23:13:57.865927736Z",
     "start_time": "2024-03-08T23:13:57.855689418Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query_model(prompt , number_responses=1):\n",
    "    url = 'http://127.0.0.1:5000/generate'\n",
    "    \n",
    "    response = requests.post(url, json={'prompt': prompt, 'number_responses' : number_responses })\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('responses')\n",
    "    else:\n",
    "        return \"Error: \" + response.text\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T23:13:57.905830612Z",
     "start_time": "2024-03-08T23:13:57.860733056Z"
    }
   },
   "id": "aa7d6fe0a907330f",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3518026a-eb42-4f98-b6f3-558c02d83135",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-03-08T23:13:57.907406668Z",
     "start_time": "2024-03-08T23:13:57.903526165Z"
    }
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import transformers\n",
    "# import torch\n",
    "# \n",
    "# \n",
    "# model_id = \"google/gemma-7b-it\"\n",
    "# dtype = torch.bfloat16\n",
    "# \n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=\"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\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map=\"cuda\",\n",
    "#     torch_dtype=dtype,\n",
    "#     use_auth_token=\"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\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # Apply chat template and encode the input\n",
    "# prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "# # Generate a response from the model\n",
    "# outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=300)\n",
    "\n",
    "# # Decode the generated response\n",
    "# response = tokenizer.decode(outputs[0])\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f159e97-e936-42b2-8b5b-da988f27b425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T23:13:57.965978958Z",
     "start_time": "2024-03-08T23:13:57.909249443Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "EUREKA_ROOT_DIR = os.getcwd()\n",
    "ISAAC_ROOT_DIR = f\"{EUREKA_ROOT_DIR}/../isaacgymenvs/isaacgymenvs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00b7bbbe-491a-4285-a3ec-049da64d8dd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T23:13:58.163112280Z",
     "start_time": "2024-03-08T23:13:57.955114902Z"
    }
   },
   "outputs": [],
   "source": [
    "from hydra import initialize, compose\n",
    "\n",
    "initialize(config_path=\"cfg\", version_base=\"1.1\")\n",
    "cfg = compose(config_name=\"config\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d31df66-5f54-4654-b6dc-c6056fe3a2b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T23:13:58.608830938Z",
     "start_time": "2024-03-08T23:13:58.592618620Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "workspace_dir = Path.cwd()\n",
    "logging.info(f\"Workspace: {workspace_dir}\")\n",
    "logging.info(f\"Project Root: {EUREKA_ROOT_DIR}\")\n",
    "task = cfg.env.task\n",
    "task_description = cfg.env.description\n",
    "suffix = cfg.suffix\n",
    "model = 'gemma-7b-it'\n",
    "logging.info(f\"Using LLM: {model}\")\n",
    "logging.info(\"Task: \" + task)\n",
    "logging.info(\"Task description: \" + task_description)\n",
    "env_name = cfg.env.env_name.lower()\n",
    "env_parent = 'isaac' if f'{env_name}.py' in os.listdir(f'{EUREKA_ROOT_DIR}/envs/isaac') else 'dexterity'\n",
    "task_file = f'{EUREKA_ROOT_DIR}/envs/{env_parent}/{env_name}.py'\n",
    "task_obs_file = f'{EUREKA_ROOT_DIR}/envs/{env_parent}/{env_name}_obs.py'\n",
    "shutil.copy(task_obs_file, f\"env_init_obs.py\")\n",
    "task_code_string  = file_to_string(task_file) # it \n",
    "task_obs_code_string  = file_to_string(task_obs_file)\n",
    "output_file = f\"{ISAAC_ROOT_DIR}/tasks/{env_name}{suffix.lower()}.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281a4416-d38a-42fa-bf66-87fed30ff248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T23:13:59.062406642Z",
     "start_time": "2024-03-08T23:13:59.055739237Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55258ec0-7786-4ce4-b2a9-eef77f1bdc90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T23:13:59.337585410Z",
     "start_time": "2024-03-08T23:13:59.331337344Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading all text prompts\n",
    "prompt_dir = f'{EUREKA_ROOT_DIR}/utils/prompts'\n",
    "initial_system = file_to_string(f'{prompt_dir}/initial_system.txt')\n",
    "code_output_tip = file_to_string(f'{prompt_dir}/code_output_tip.txt')\n",
    "code_feedback = file_to_string(f'{prompt_dir}/code_feedback.txt')\n",
    "initial_user = file_to_string(f'{prompt_dir}/initial_user.txt')\n",
    "reward_signature = file_to_string(f'{prompt_dir}/reward_signature.txt')\n",
    "policy_feedback = file_to_string(f'{prompt_dir}/policy_feedback.txt')\n",
    "execution_error_feedback = file_to_string(f'{prompt_dir}/execution_error_feedback.txt')\n",
    "\n",
    "initial_system = \"Background information or context: \" + initial_system.format(task_reward_signature_string=reward_signature) + code_output_tip\n",
    "initial_user = initial_user.format(task_obs_code_string=task_obs_code_string, task_description=task_description)\n",
    "combined_user_message = initial_system + \"\\n\" + initial_user\n",
    "messages = [{\"role\": \"user\", \"content\": combined_user_message}]\n",
    "\n",
    "task_code_string = task_code_string.replace(task, task+suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0340c97c-2608-410c-b2c4-1e3f0c36d496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T23:13:59.878907548Z",
     "start_time": "2024-03-08T23:13:59.849687110Z"
    }
   },
   "outputs": [],
   "source": [
    "create_task(ISAAC_ROOT_DIR, cfg.env.task, cfg.env.env_name, suffix)\n",
    "\n",
    "DUMMY_FAILURE = -10000.\n",
    "max_successes = []\n",
    "max_successes_reward_correlation = []\n",
    "execute_rates = []\n",
    "best_code_paths = []\n",
    "max_success_overall = DUMMY_FAILURE\n",
    "max_success_reward_correlation_overall = DUMMY_FAILURE\n",
    "max_reward_code_path = None \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6124e13-f5ef-4072-bbc0-5ceabc4e75ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T23:14:00.419651652Z",
     "start_time": "2024-03-08T23:14:00.410922218Z"
    }
   },
   "outputs": [],
   "source": [
    "iter=0"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "responses = query_model(messages, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T23:14:44.274938603Z",
     "start_time": "2024-03-08T23:14:20.803061245Z"
    }
   },
   "id": "c37822317d0944d",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "[INST] Background information or context: You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\n",
      "Your goal is to write a reward function for the environment that will help the agent learn the task described in text. \n",
      "Your reward function should use useful variables from the environment as inputs. As an example,\n",
      "the reward function signature can be: @torch.jit.script\n",
      "def compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "    ...\n",
      "    return reward, {}\n",
      "\n",
      "Since the reward function will be decorated with @torch.jit.script,\n",
      "please make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \n",
      "Make sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n",
      "    (1) the total reward,\n",
      "    (2) a dictionary of each individual reward component.\n",
      "The code output should be formatted as a python code string: \"```python ... ```\".\n",
      "\n",
      "Some helpful tips for writing the reward function code:\n",
      "    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n",
      "    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n",
      "    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n",
      "    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n",
      "\n",
      "The Python environment is class Cartpole(VecTask):\n",
      "    \"\"\"Rest of the environment definition omitted.\"\"\"\n",
      "    def compute_observations(self, env_ids=None):\n",
      "        if env_ids is None:\n",
      "            env_ids = np.arange(self.num_envs)\n",
      "\n",
      "        self.gym.refresh_dof_state_tensor(self.sim)\n",
      "\n",
      "        self.obs_buf[env_ids, 0] = self.dof_pos[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 1] = self.dof_vel[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 2] = self.dof_pos[env_ids, 1].squeeze()\n",
      "        self.obs_buf[env_ids, 3] = self.dof_vel[env_ids, 1].squeeze()\n",
      "\n",
      "        return self.obs_buf\n",
      ". Write a reward function for the following task: to balance a pole on a cart so that the pole stays upright. [/INST] ```python\n",
      "import torch\n",
      "\n",
      "class CartpoleReward(torch.nn.Module):\n",
      "    def __init__(self):\n",
      "        super(CartpoleReward, self).__init__()\n",
      "\n",
      "    @torch.jit.script\n",
      "    def forward(self, cart_position: torch.Tensor, pole_angle: torch.Tensor, pole_velocity: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "        # The goal is to keep the pole upright, so we want to penalize large angles and large velocity\n",
      "        angle_penalty = -torch.cos(pole_angle) ** 2  # negative cosine squared of pole angle as penalty\n",
      "        angle_velocity_penalty = torch.abs(pole_velocity)  # absolute value of pole velocity as penalty\n",
      "        total_penalty = angle_penalty + angle_velocity_penalty\n",
      "         # Reward the agent for position close to the center and small velocity\n",
      "        position_reward = 1 - torch.abs(cart_position)\n",
      "        velocity_reward = torch.abs(cart_velocity) * 0.1  # make velocity reward smaller than position reward\n",
      "        total_reward = position_reward + velocity_reward\n",
      "        # Normalize rewards to have the same scale for both components\n",
      "        angle_penalty /= torch.max(angle_penalty)\n",
      "        angle_velocity_penalty /= torch.max(angle_velocity_penalty)\n",
      "        total_penalty /= torch.max(total_penalty)\n",
      "        total_reward /= torch.max(total_reward)\n",
      "\n",
      "        return total_reward, {\n",
      "            'angle_penalty': angle_penalty,\n",
      "            'angle_velocity_penalty': angle_velocity_penalty,\n",
      "            'total_penalty': total_penalty,\n",
      "            'position_reward': position_reward,\n",
      "            'velocity_reward': velocity_reward,\n",
      "        }\n",
      "\n",
      "cartpole_reward = CartpoleReward()\n",
      "```\n",
      "\n",
      "In this code snippet, the reward function is defined as a `torch.nn.Module` class called `CartpoleReward`, with a single forward method. The input to the forward method is the cart position, pole angle, and pole velocity. The reward is calculated by taking a negative cosine squared of the pole angle as an angle penalty, the absolute value of the pole velocity as a velocity penalty, and summing them up as the total penalty. We also reward the agent for position close to the center and small velocity. The rewards are normalized to have the same scale for both components.\n",
      "\n",
      "The output of the forward method is a tuple with the total reward and a dictionary containing each individual reward component, making it compatible with the given reward function signature.\n",
      "\n",
      "Response 2:\n",
      "[INST] Background information or context: You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\n",
      "Your goal is to write a reward function for the environment that will help the agent learn the task described in text. \n",
      "Your reward function should use useful variables from the environment as inputs. As an example,\n",
      "the reward function signature can be: @torch.jit.script\n",
      "def compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "    ...\n",
      "    return reward, {}\n",
      "\n",
      "Since the reward function will be decorated with @torch.jit.script,\n",
      "please make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \n",
      "Make sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n",
      "    (1) the total reward,\n",
      "    (2) a dictionary of each individual reward component.\n",
      "The code output should be formatted as a python code string: \"```python ... ```\".\n",
      "\n",
      "Some helpful tips for writing the reward function code:\n",
      "    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n",
      "    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n",
      "    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n",
      "    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n",
      "\n",
      "The Python environment is class Cartpole(VecTask):\n",
      "    \"\"\"Rest of the environment definition omitted.\"\"\"\n",
      "    def compute_observations(self, env_ids=None):\n",
      "        if env_ids is None:\n",
      "            env_ids = np.arange(self.num_envs)\n",
      "\n",
      "        self.gym.refresh_dof_state_tensor(self.sim)\n",
      "\n",
      "        self.obs_buf[env_ids, 0] = self.dof_pos[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 1] = self.dof_vel[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 2] = self.dof_pos[env_ids, 1].squeeze()\n",
      "        self.obs_buf[env_ids, 3] = self.dof_vel[env_ids, 1].squeeze()\n",
      "\n",
      "        return self.obs_buf\n",
      ". Write a reward function for the following task: to balance a pole on a cart so that the pole stays upright. [/INST] ```python\n",
      "import torch\n",
      "\n",
      "class CartpoleReward:\n",
      "    def __init__(self, discount_factor=0.99, pole_length=1.0, gravity=-9.8):\n",
      "        self.discount_factor = discount_factor\n",
      "        self.pole_length = pole_length\n",
      "        self.gravity = gravity\n",
      "\n",
      "    @torch.jit.script\n",
      "    def compute_reward(self, cart_pos: torch.Tensor, pole_angle: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "        # Compute individual reward components\n",
      "        x_reward = -(torch.abs(cart_pos) - 1.2) * 5.0\n",
      "        pole_angle_reward = torch.cos(pole_angle) * 5.0\n",
      "        pole_velocity_reward = -pole_angle.abs() * 5.0\n",
      "        collision_penalty = torch.where(cart_pos.abs() > 2.4, -100., torch.tensor(0.))\n",
      "\n",
      "        # Normalize rewards using exponential transformation\n",
      "        x_reward = torch.exp(x_reward)\n",
      "        pole_angle_reward = torch.exp(pole_angle_reward)\n",
      "        pole_velocity_reward = torch.exp(pole_velocity_reward)\n",
      "\n",
      "        # Total reward\n",
      "        reward = x_reward + pole_angle_reward - pole_velocity_reward\n",
      "\n",
      "        # Dictionary of individual reward components\n",
      "        rewards = {\n",
      "            'x_reward': x_reward,\n",
      "            'pole_angle_reward': pole_angle_reward,\n",
      "            'pole_velocity_reward': pole_velocity_reward,\n",
      "            'collision_penalty': collision_penalty\n",
      "        }\n",
      "\n",
      "        return reward, rewards\n",
      "```\n",
      "\n",
      "This reward function encourages the agent to keep the cart position within the range of [-1.2, 1.2] and maintain the pole angle close to the upright position while minimizing pole velocity. A collision with the environment results in a large negative penalty. All reward components are normalized using exponential transformation.\n",
      "\n",
      "Response 3:\n",
      "[INST] Background information or context: You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\n",
      "Your goal is to write a reward function for the environment that will help the agent learn the task described in text. \n",
      "Your reward function should use useful variables from the environment as inputs. As an example,\n",
      "the reward function signature can be: @torch.jit.script\n",
      "def compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "    ...\n",
      "    return reward, {}\n",
      "\n",
      "Since the reward function will be decorated with @torch.jit.script,\n",
      "please make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \n",
      "Make sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n",
      "    (1) the total reward,\n",
      "    (2) a dictionary of each individual reward component.\n",
      "The code output should be formatted as a python code string: \"```python ... ```\".\n",
      "\n",
      "Some helpful tips for writing the reward function code:\n",
      "    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n",
      "    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n",
      "    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n",
      "    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n",
      "\n",
      "The Python environment is class Cartpole(VecTask):\n",
      "    \"\"\"Rest of the environment definition omitted.\"\"\"\n",
      "    def compute_observations(self, env_ids=None):\n",
      "        if env_ids is None:\n",
      "            env_ids = np.arange(self.num_envs)\n",
      "\n",
      "        self.gym.refresh_dof_state_tensor(self.sim)\n",
      "\n",
      "        self.obs_buf[env_ids, 0] = self.dof_pos[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 1] = self.dof_vel[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 2] = self.dof_pos[env_ids, 1].squeeze()\n",
      "        self.obs_buf[env_ids, 3] = self.dof_vel[env_ids, 1].squeeze()\n",
      "\n",
      "        return self.obs_buf\n",
      ". Write a reward function for the following task: to balance a pole on a cart so that the pole stays upright. [/INST] ```python\n",
      "import torch\n",
      "\n",
      "class CartpoleReward:\n",
      "    def __init__(self, discount_factor=0.99):\n",
      "        self.discount_factor = discount_ factor\n",
      "\n",
      "    @torch.jit.script\n",
      "    def compute_reward(self, cart_position: torch.Tensor, pole_angle: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "        # Define reward components\n",
      "        position_reward = -5. * (torch.abs(cart_position) - 1.)\n",
      "        angle_reward = torch.cos(pole_angle)\n",
      "\n",
      "        # Apply exponential transformation to position reward with temperature 0.1\n",
      "        position_reward = torch.exp(-0.1 * position_reward)\n",
      "\n",
      "        # Compute total reward as the sum of position and angle reward components\n",
      "        total_reward = position_reward + angle_reward\n",
      "\n",
      "        # Normalize reward to range [0, 1] using discounting\n",
      "        total_reward = self.discount_factor * total_reward.sum(dim=0)\n",
      "\n",
      "        # Define and return the reward components dictionary\n",
      "        reward_components = {\"position_reward\": position_reward, \"angle_reward\": angle_reward}\n",
      "        return total_reward, reward_components\n",
      "```\n",
      "\n",
      "This reward function calculates a reward that encourages the agent to keep the cart position close to zero and the pole angle close to upright (around 0 radians). It applies an exponential transformation to the position reward component with a temperature parameter of 0.1 to add some noise and make the learning process more stochastic. The final reward is the sum of the position and angle reward components, which is then discounted and normalized to the range [0, 1].\n",
      "\n",
      "Response 4:\n",
      "[INST] Background information or context: You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\n",
      "Your goal is to write a reward function for the environment that will help the agent learn the task described in text. \n",
      "Your reward function should use useful variables from the environment as inputs. As an example,\n",
      "the reward function signature can be: @torch.jit.script\n",
      "def compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "    ...\n",
      "    return reward, {}\n",
      "\n",
      "Since the reward function will be decorated with @torch.jit.script,\n",
      "please make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \n",
      "Make sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n",
      "    (1) the total reward,\n",
      "    (2) a dictionary of each individual reward component.\n",
      "The code output should be formatted as a python code string: \"```python ... ```\".\n",
      "\n",
      "Some helpful tips for writing the reward function code:\n",
      "    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n",
      "    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n",
      "    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n",
      "    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n",
      "\n",
      "The Python environment is class Cartpole(VecTask):\n",
      "    \"\"\"Rest of the environment definition omitted.\"\"\"\n",
      "    def compute_observations(self, env_ids=None):\n",
      "        if env_ids is None:\n",
      "            env_ids = np.arange(self.num_envs)\n",
      "\n",
      "        self.gym.refresh_dof_state_tensor(self.sim)\n",
      "\n",
      "        self.obs_buf[env_ids, 0] = self.dof_pos[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 1] = self.dof_vel[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 2] = self.dof_pos[env_ids, 1].squeeze()\n",
      "        self.obs_buf[env_ids, 3] = self.dof_vel[env_ids, 1].squeeze()\n",
      "\n",
      "        return self.obs_buf\n",
      ". Write a reward function for the following task: to balance a pole on a cart so that the pole stays upright. [/INST] Here is an example of a reward function for the Cartpole environment that aims to keep the pole upright:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "\n",
      "class CartpoleReward(torch.nn.Module):\n",
      "    def __init__(self):\n",
      "        super(CartpoleReward, self).__init__()\n",
      "\n",
      "    @torch.jit.script\n",
      "    def forward(self, cart_position: torch.Tensor, cart_velocity: torch.Tensor, pole_angle: torch.Tensor, pole_angular_velocity: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "        xt = torch.cat((cart_position, cart_velocity, pole_angle, pole_angular_velocity), dim=-1)\n",
      "\n",
      "        # Apply exponential function to limit rewards and make them positive\n",
      "        reward = torch.exp(-0.01 * (torch.abs(cart_position) + torch.abs(cart_velocity) + torch.abs(pole_angle)))\n",
      "\n",
      "        # Define individual reward components\n",
      "        position_reward = (cart_position <= 0.5).float() * 5.0 # Reward for being within the boundary (+5.0 for pole balanced)\n",
      "        velocity_penalty = torch.abs(cart_velocity)\n",
      "        angle_penalty = torch.abs(pole_angle)\n",
      "\n",
      "        # Sum up the individual reward components\n",
      "        total_reward = reward * (position_reward - velocity_penalty - angle_penalty)\n",
      "\n",
      "        return total_reward, {\n",
      "            \"position_reward\": position_reward,\n",
      "            \"velocity_penalty\": velocity_penalty,\n",
      "            \"angle_penalty\": angle_penalty\n",
      "        }\n",
      "```\n",
      "\n",
      "This reward function, `CartpoleReward`, takes cart position, cart velocity, pole angle, and pole angular velocity as input and returns the total reward and a dictionary containing individual reward components. The reward function uses the `exp` function to limit the rewards and makes them positive. The position reward is given when the cart is within the boundary (left = -0.5, right = 0.5), with an additional bonus (+5.0) for keeping the pole upright. The velocity and angle penalty are simple abs(value). The individual reward components are then summed up and multiplied with the base reward to result in the total reward. Since all values are given as tensors, this function is compatible with TorchScript.\n"
     ]
    }
   ],
   "source": [
    "for i, response in enumerate(responses):\n",
    "    print(f\"Response {i+1}:\\n{response}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T23:14:44.318699463Z",
     "start_time": "2024-03-08T23:14:44.316914302Z"
    }
   },
   "id": "f2d5aa773f631b20",
   "execution_count": 13
  },
  {
   "cell_type": "raw",
   "id": "c551647e-ec9e-4368-b8ef-5f803eda219f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18488eee-8be9-45c0-9436-e2029b7db8db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T23:17:14.877401789Z",
     "start_time": "2024-03-08T23:16:16.157508084Z"
    }
   },
   "outputs": [],
   "source": [
    "# for iter in range(cfg.iteration):\n",
    "    # Get Eureka response\n",
    "responses = []\n",
    "response_cur = None\n",
    "total_samples = 0\n",
    "total_token = 0\n",
    "total_completion_token = 0\n",
    "chunk_size = 4\n",
    "\n",
    "logging.info(f\"Iteration {iter}: Generating {cfg.sample} samples with {cfg.model}\")\n",
    "while True:\n",
    "    if total_samples >= cfg.sample:\n",
    "        break\n",
    "    for attempt in range(1000):\n",
    "        try:\n",
    "            responses = query_model(messages, chunk_size)\n",
    "            total_samples += chunk_size\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt >= 10:\n",
    "                chunk_size = max(int(chunk_size / 2), 1)\n",
    "                print(\"Current Chunk Size\", chunk_size)\n",
    "            logging.info(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            time.sleep(1)\n",
    "    if len(responses) == 0:\n",
    "        logging.info(\"Code terminated due to too many failed attempts!\")\n",
    "        exit()\n",
    "    \n",
    "# if cfg.sample == 1:\n",
    "#     logging.info(f\"Iteration {iter}: GPT Output:\\n \" + responses[0][\"message\"][\"content\"] + \"\\n\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d39ffdf9-2e51-4538-a27d-af617d068021",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T23:17:14.927107990Z",
     "start_time": "2024-03-08T23:17:14.920929718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "[INST] Background information or context: You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\n",
      "Your goal is to write a reward function for the environment that will help the agent learn the task described in text. \n",
      "Your reward function should use useful variables from the environment as inputs. As an example,\n",
      "the reward function signature can be: @torch.jit.script\n",
      "def compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "    ...\n",
      "    return reward, {}\n",
      "\n",
      "Since the reward function will be decorated with @torch.jit.script,\n",
      "please make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \n",
      "Make sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n",
      "    (1) the total reward,\n",
      "    (2) a dictionary of each individual reward component.\n",
      "The code output should be formatted as a python code string: \"```python ... ```\".\n",
      "\n",
      "Some helpful tips for writing the reward function code:\n",
      "    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n",
      "    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n",
      "    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n",
      "    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n",
      "\n",
      "The Python environment is class Cartpole(VecTask):\n",
      "    \"\"\"Rest of the environment definition omitted.\"\"\"\n",
      "    def compute_observations(self, env_ids=None):\n",
      "        if env_ids is None:\n",
      "            env_ids = np.arange(self.num_envs)\n",
      "\n",
      "        self.gym.refresh_dof_state_tensor(self.sim)\n",
      "\n",
      "        self.obs_buf[env_ids, 0] = self.dof_pos[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 1] = self.dof_vel[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 2] = self.dof_pos[env_ids, 1].squeeze()\n",
      "        self.obs_buf[env_ids, 3] = self.dof_vel[env_ids, 1].squeeze()\n",
      "\n",
      "        return self.obs_buf\n",
      ". Write a reward function for the following task: to balance a pole on a cart so that the pole stays upright. [/INST] ```python\n",
      "import torch\n",
      "\n",
      "class CartpoleReward(torch.nn.Module):\n",
      "    def __init__(self):\n",
      "        super(CartpoleReward, self).__init__()\n",
      "\n",
      "    @torch.jit.script\n",
      "    def forward(self, cart_pos: torch.Tensor, pole_angle: torch.Tensor, pole_angular_vel: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "        # Set temperature parameters for reward transformations\n",
      "        angle_temp = 0.1\n",
      "        velocity_temp = 0.1\n",
      "\n",
      "        # Compute individual reward components\n",
      "        position_reward = torch.clamp((self.max_position - torch.abs(cart_pos)), min=0.0)\n",
      "        angle_reward = torch.clamp((self.max_angle - torch.abs(pole_angle)), min=0.0)\n",
      "        velocity_penalty = torch.abs(pole_angular_vel)\n",
      "\n",
      "        # Apply exponential transformations to individual reward components\n",
      "        position_reward = torch.exp(-position_reward / position_temp)\n",
      "        angle_reward = torch.exp(-angle_reward / angle_temp)\n",
      "        velocity_penalty = torch.exp(-velocity_penalty / velocity_temp)\n",
      "\n",
      "        # Combine individual reward components to compute the total reward\n",
      "        total_reward = position_reward + angle_reward - velocity_penalty\n",
      "\n",
      "        # Return the total reward and the dictionary of individual reward components\n",
      "        return total_reward, {\n",
      "            'position_reward': position_reward,\n",
      "            'angle_reward': angle_reward,\n",
      "            'velocity_penalty': velocity_penalty\n",
      "        }\n",
      "\n",
      "# Initialize the reward function as a module\n",
      "reward_fn = CartpoleReward()\n",
      "\n",
      "# Define maximum position and angle limits\n",
      "CartpoleReward.max_position = 2.4\n",
      "CartpoleReward.max_angle = 12.0\n",
      "```\n",
      "\n",
      "In this code, we define a `CartpoleReward` class that inherits from PyTorch's `nn.Module`. Inside the class, there's a forward method decorated with `@torch.jit.script`. We use this decorator since the reward function will be used inside TorchScript. The forward method takes the cart position, pole angle, and pole angular velocity as inputs. We also define maximum position and angle limits as class variables.\n",
      "\n",
      "The method computes individual reward components like the position, angle, and velocity penalties. It then applies exponential transformations to the reward components, setting temperature parameters for the transformations. Finally, the method combines the individual reward components and computes the total reward.\n",
      "\n",
      "After initializing the reward function, the provided maximum position and angle limits are set as class variables. These variables are used inside the reward calculation when computing the position and angle reward components.\n",
      "\n",
      "Response 2:\n",
      "[INST] Background information or context: You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\n",
      "Your goal is to write a reward function for the environment that will help the agent learn the task described in text. \n",
      "Your reward function should use useful variables from the environment as inputs. As an example,\n",
      "the reward function signature can be: @torch.jit.script\n",
      "def compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "    ...\n",
      "    return reward, {}\n",
      "\n",
      "Since the reward function will be decorated with @torch.jit.script,\n",
      "please make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \n",
      "Make sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n",
      "    (1) the total reward,\n",
      "    (2) a dictionary of each individual reward component.\n",
      "The code output should be formatted as a python code string: \"```python ... ```\".\n",
      "\n",
      "Some helpful tips for writing the reward function code:\n",
      "    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n",
      "    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n",
      "    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n",
      "    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n",
      "\n",
      "The Python environment is class Cartpole(VecTask):\n",
      "    \"\"\"Rest of the environment definition omitted.\"\"\"\n",
      "    def compute_observations(self, env_ids=None):\n",
      "        if env_ids is None:\n",
      "            env_ids = np.arange(self.num_envs)\n",
      "\n",
      "        self.gym.refresh_dof_state_tensor(self.sim)\n",
      "\n",
      "        self.obs_buf[env_ids, 0] = self.dof_pos[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 1] = self.dof_vel[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 2] = self.dof_pos[env_ids, 1].squeeze()\n",
      "        self.obs_buf[env_ids, 3] = self.dof_vel[env_ids, 1].squeeze()\n",
      "\n",
      "        return self.obs_buf\n",
      ". Write a reward function for the following task: to balance a pole on a cart so that the pole stays upright. [/INST] ```python\n",
      "import torch\n",
      "\n",
      "class CartpoleReward:\n",
      "    def compute_reward(self, obj_pos, goal_pos):\n",
      "        # The goal_pos is not provided in the environment class,assuming it's the zero position\n",
      "        # Adjust the reward based on the pole angle\n",
      "        pole_angle = self.dof_pos[0, 2]\n",
      "        deadzone = 15.0  # degrees\n",
      "        max_reward = 1.0\n",
      "\n",
      "        reward = max_reward * (1 - abs(pole_angle) / deadzone)\n",
      "\n",
      "        # Add penalty for cart position deviation\n",
      "        cart_position = obj_pos[2]\n",
      "        position_penalty = -0.1 * abs(cart_position)\n",
      "        reward -= position_penalty\n",
      "\n",
      "        # Add penalty for cart velocity\n",
      "        cart_velocity = obj_pos[3]\n",
      "        velocity_penalty = -0.01 * abs(cart_velocity)\n",
      "        reward -= velocity_penalty\n",
      "\n",
      "        return reward, {\"angle_reward\": reward, \"position_penalty\": position_penalty, \"velocity_penalty\": velocity_penalty}\n",
      "```\n",
      "\n",
      "This reward function encourages the agent to balance the pole by maintaining a small angle, while penalizing deviations in the cart's position and velocity. The temperature parameters for any transformations are not provided in this example, as the given task does not require such transformations.\n",
      "\n",
      "Response 3:\n",
      "[INST] Background information or context: You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\n",
      "Your goal is to write a reward function for the environment that will help the agent learn the task described in text. \n",
      "Your reward function should use useful variables from the environment as inputs. As an example,\n",
      "the reward function signature can be: @torch.jit.script\n",
      "def compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "    ...\n",
      "    return reward, {}\n",
      "\n",
      "Since the reward function will be decorated with @torch.jit.script,\n",
      "please make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \n",
      "Make sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n",
      "    (1) the total reward,\n",
      "    (2) a dictionary of each individual reward component.\n",
      "The code output should be formatted as a python code string: \"```python ... ```\".\n",
      "\n",
      "Some helpful tips for writing the reward function code:\n",
      "    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n",
      "    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n",
      "    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n",
      "    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n",
      "\n",
      "The Python environment is class Cartpole(VecTask):\n",
      "    \"\"\"Rest of the environment definition omitted.\"\"\"\n",
      "    def compute_observations(self, env_ids=None):\n",
      "        if env_ids is None:\n",
      "            env_ids = np.arange(self.num_envs)\n",
      "\n",
      "        self.gym.refresh_dof_state_tensor(self.sim)\n",
      "\n",
      "        self.obs_buf[env_ids, 0] = self.dof_pos[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 1] = self.dof_vel[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 2] = self.dof_pos[env_ids, 1].squeeze()\n",
      "        self.obs_buf[env_ids, 3] = self.dof_vel[env_ids, 1].squeeze()\n",
      "\n",
      "        return self.obs_buf\n",
      ". Write a reward function for the following task: to balance a pole on a cart so that the pole stays upright. [/INST] ```python\n",
      "import torch\n",
      "\n",
      "class CartpoleReward:\n",
      "    def compute_reward(self, state: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "        x_pos, x_vel, theta_pos, theta_vel = state.unbind(dim=1)\n",
      "\n",
      "        # Penalize cart position deviation from 0\n",
      "        position_reward = -torch.abs(x_pos - 0.5)\n",
      "\n",
      "        # Penalize pole angle deviation from vertical\n",
      "        angle_reward = torch.cos(theta_pos) * 10.0\n",
      "\n",
      "        # Penalize high cart velocity\n",
      "        high_velocity_penalty = torch.exp(-0.01 * torch.abs(x_vel))\n",
      "\n",
      "        # Penalize high pole angle deviation from vertical\n",
      "        high_angle_penalty = torch.exp(-0.1 * torch.abs(theta_vel))\n",
      "\n",
      "        # Apply temperature to velocity and angle penalites for smoother learning\n",
      "        vel_temp = 0.1\n",
      "        angle_temp = 0.5\n",
      "\n",
      "        velocity_reward = (1 - high_velocity_penalty) * position_reward\n",
      "\n",
      "        angle_reward = (1 - high_angle_penalty) * (angle_reward + velocity_reward)\n",
      "\n",
      "        total_reward = angle_reward\n",
      "\n",
      "        return total_reward, {\n",
      "            \"position_reward\": position_reward,\n",
      "            \"angle_reward\": angle_reward,\n",
      "            \"velocity_reward\": velocity_reward,\n",
      "            \"high_velocity_penalty\": high_velocity_penalty,\n",
      "            \"high_angle_penalty\": high_angle_penalty\n",
      "        }\n",
      "```\n",
      "\n",
      "This reward function defines a class `CartpoleReward` that uses the provided `state` tensor as input. The state tensor is unbound into separate tensors for cart position, cart velocity, pole position, and pole velocity. The reward function then applies penalties to the cart position deviation and pole angle deviation from their respective targets. Additionally, it applies exponential decay penalties to high cart velocity and pole angle velocities with separate temperature parameters. The final reward is the sum of the angle reward and a scaled velocity reward. This implementation ensures that the agent learns to keep the cart at the center and the pole upright.\n",
      "\n",
      "Response 4:\n",
      "[INST] Background information or context: You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\n",
      "Your goal is to write a reward function for the environment that will help the agent learn the task described in text. \n",
      "Your reward function should use useful variables from the environment as inputs. As an example,\n",
      "the reward function signature can be: @torch.jit.script\n",
      "def compute_reward(object_pos: torch.Tensor, goal_pos: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "    ...\n",
      "    return reward, {}\n",
      "\n",
      "Since the reward function will be decorated with @torch.jit.script,\n",
      "please make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). \n",
      "Make sure any new tensor or variable you introduce is on the same device as the input tensors. The output of the reward function should consist of two items:\n",
      "    (1) the total reward,\n",
      "    (2) a dictionary of each individual reward component.\n",
      "The code output should be formatted as a python code string: \"```python ... ```\".\n",
      "\n",
      "Some helpful tips for writing the reward function code:\n",
      "    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components\n",
      "    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable\n",
      "    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor\n",
      "    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.\n",
      "\n",
      "The Python environment is class Cartpole(VecTask):\n",
      "    \"\"\"Rest of the environment definition omitted.\"\"\"\n",
      "    def compute_observations(self, env_ids=None):\n",
      "        if env_ids is None:\n",
      "            env_ids = np.arange(self.num_envs)\n",
      "\n",
      "        self.gym.refresh_dof_state_tensor(self.sim)\n",
      "\n",
      "        self.obs_buf[env_ids, 0] = self.dof_pos[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 1] = self.dof_vel[env_ids, 0].squeeze()\n",
      "        self.obs_buf[env_ids, 2] = self.dof_pos[env_ids, 1].squeeze()\n",
      "        self.obs_buf[env_ids, 3] = self.dof_vel[env_ids, 1].squeeze()\n",
      "\n",
      "        return self.obs_buf\n",
      ". Write a reward function for the following task: to balance a pole on a cart so that the pole stays upright. [/INST] ```python\n",
      "import torch\n",
      "\n",
      "class CartpoleReward(torch.nn.Module):\n",
      "    def __init__(self):\n",
      "        super(CartpoleReward, self).__init__()\n",
      "\n",
      "    @torch.jit.script\n",
      "    def forward(self, cart_position: torch.Tensor, pole_angle: torch.Tensor, pole_angular_velocity: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "        # Define individual reward components\n",
      "        position_reward = torch.tensor(self.position_reward, requires_grad=False).to(cart_position.device)\n",
      "        angle_reward = torch.tensor(self.angle_reward, requires_grad=False).to(pole_angle.device)\n",
      "        velocity_penalty = torch.tensor(self.velocity_penalty, requires_grad=False).to(pole_angular_velocity.device)\n",
      "\n",
      "        total_reward = 0.0\n",
      "\n",
      "        # Reward for keeping the cart position within the bounds\n",
      "        total_reward += position_reward * (cart_position.abs().sum(axis=-1) < self.bound)\n",
      "\n",
      "        # Reward for keeping the pole upright\n",
      "        total_reward += angle_reward * (pole_angle.abs().sum(axis=-1) < self.pole_limit)\n",
      "\n",
      "        # Penalty for high pole angular velocities\n",
      "        total_reward -= velocity_penalty * (pole_angular_velocity.pow(2).sum(axis=-1))\n",
      "\n",
      "        # Normalize and combine the reward components\n",
      "        reward = total_reward / (3. * self.num_envs)\n",
      "        position_reward_component = total_reward * position_reward / (3. * self.num_envs)\n",
      "        angle_reward_component = total_reward * angle_reward / (3. * self.num_envs)\n",
      "        velocity_penalty_component = total_reward * (-velocity_penalty) / (3. * self.num_envs)\n",
      "\n",
      "        return reward, {\n",
      "            'position_reward': position_reward_component,\n",
      "            'angle_reward': angle_reward_component,\n",
      "            'velocity_penalty': velocity_penalty_component\n",
      "        }\n",
      "\n",
      "    @property\n",
      "    def position_reward(self):\n",
      "        return 0.1\n",
      "\n",
      "    @property\n",
      "    def angle_reward(self):\n",
      "        return 0.04\n",
      "\n",
      "    @property\n",
      "    def velocity_penalty(self):\n",
      "        return 0.01\n",
      "\n",
      "    @property\n",
      "    def bound(self):\n",
      "        return 2.4\n",
      "\n",
      "    @property\n",
      "    def pole_limit(self):\n",
      "        return 12.\n",
      "\n",
      "    @property\n",
      "    def num_envs(self):\n",
      "        return self.sim.numJoints\n",
      "```\n",
      "\n",
      "In this example, I've turned the reward function into a PyTorch module, as it might be more convenient to write and reason about the reward function in this form. This approach makes it easy to write and use reward functions with the provided inputs (cart_position, pole_angle, and pole_angular_velocity), compute the total reward and its individual components, and normalize these components. Additionally, I've used property decorators to define constant values rather than hardcode them into the function.\n",
      "\n",
      "Please ensure that any constant values used in the reward function (like the bound, pole_limit, position_reward, angle_reward, and velocity_penalty) match your environment requirements.\n"
     ]
    }
   ],
   "source": [
    "for i, response in enumerate(responses):\n",
    "    print(f\"Response {i+1}:\\n{response}\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01d4e356-5386-4e54-98be-a43f3b0b2e67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T23:29:17.557347446Z",
     "start_time": "2024-03-08T23:29:17.512651296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def forward(self, cart_pos: torch.Tensor, pole_angle: torch.Tensor, pole_angular_vel: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "        # Set temperature parameters for reward transformations\n",
      "        angle_temp = 0.1\n",
      "        velocity_temp = 0.1\n",
      "\n",
      "        # Compute individual reward components\n",
      "        position_reward = torch.clamp((self.max_position - torch.abs(cart_pos)), min=0.0)\n",
      "        angle_reward = torch.clamp((self.max_angle - torch.abs(pole_angle)), min=0.0)\n",
      "        velocity_penalty = torch.abs(pole_angular_vel)\n",
      "\n",
      "        # Apply exponential transformations to individual reward components\n",
      "        position_reward = torch.exp(-position_reward / position_temp)\n",
      "        angle_reward = torch.exp(-angle_reward / angle_temp)\n",
      "        velocity_penalty = torch.exp(-velocity_penalty / velocity_temp)\n",
      "\n",
      "        # Combine individual reward components to compute the total reward\n",
      "        total_reward = position_reward + angle_reward - velocity_penalty\n",
      "\n",
      "        # Return the total reward and the dictionary of individual reward components\n",
      "        return total_reward, {\n",
      "            'position_reward': position_reward,\n",
      "            'angle_reward': angle_reward,\n",
      "            'velocity_penalty': velocity_penalty\n",
      "        }\n",
      "\n",
      "# Initialize the reward function as a module\n",
      "reward_fn = CartpoleReward()\n",
      "\n",
      "# Define maximum position and angle limits\n",
      "CartpoleReward.max_position = 2.4\n",
      "CartpoleReward.max_angle = 12.0\n",
      "    def compute_reward(self, obj_pos, goal_pos):\n",
      "        # The goal_pos is not provided in the environment class,assuming it's the zero position\n",
      "        # Adjust the reward based on the pole angle\n",
      "        pole_angle = self.dof_pos[0, 2]\n",
      "        deadzone = 15.0  # degrees\n",
      "        max_reward = 1.0\n",
      "\n",
      "        reward = max_reward * (1 - abs(pole_angle) / deadzone)\n",
      "\n",
      "        # Add penalty for cart position deviation\n",
      "        cart_position = obj_pos[2]\n",
      "        position_penalty = -0.1 * abs(cart_position)\n",
      "        reward -= position_penalty\n",
      "\n",
      "        # Add penalty for cart velocity\n",
      "        cart_velocity = obj_pos[3]\n",
      "        velocity_penalty = -0.01 * abs(cart_velocity)\n",
      "        reward -= velocity_penalty\n",
      "\n",
      "        return reward, {\"angle_reward\": reward, \"position_penalty\": position_penalty, \"velocity_penalty\": velocity_penalty}\n",
      "    def compute_reward(self, state: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
      "        x_pos, x_vel, theta_pos, theta_vel = state.unbind(dim=1)\n",
      "\n",
      "        # Penalize cart position deviation from 0\n",
      "        position_reward = -torch.abs(x_pos - 0.5)\n",
      "\n",
      "        # Penalize pole angle deviation from vertical\n",
      "        angle_reward = torch.cos(theta_pos) * 10.0\n",
      "\n",
      "        # Penalize high cart velocity\n",
      "        high_velocity_penalty = torch.exp(-0.01 * torch.abs(x_vel))\n",
      "\n",
      "        # Penalize high pole angle deviation from vertical\n",
      "        high_angle_penalty = torch.exp(-0.1 * torch.abs(theta_vel))\n",
      "\n",
      "        # Apply temperature to velocity and angle penalites for smoother learning\n",
      "        vel_temp = 0.1\n",
      "        angle_temp = 0.5\n",
      "\n",
      "        velocity_reward = (1 - high_velocity_penalty) * position_reward\n",
      "\n",
      "        angle_reward = (1 - high_angle_penalty) * (angle_reward + velocity_reward)\n",
      "\n",
      "        total_reward = angle_reward\n",
      "\n",
      "        return total_reward, {\n",
      "            \"position_reward\": position_reward,\n",
      "            \"angle_reward\": angle_reward,\n",
      "            \"velocity_reward\": velocity_reward,\n",
      "            \"high_velocity_penalty\": high_velocity_penalty,\n",
      "            \"high_angle_penalty\": high_angle_penalty\n",
      "        }\n",
      "    def num_envs(self):\n",
      "        return self.sim.numJoints\n"
     ]
    }
   ],
   "source": [
    "code_runs = [] \n",
    "rl_runs = []\n",
    "for itt, response in enumerate(responses):\n",
    "    response_cur = response\n",
    "    start_index = response_cur.find(\"[/INST]\")\n",
    "    response_cur = response_cur[start_index + len(\"[/INST]\"):].strip()\n",
    "    # Regex patterns to extract python code enclosed in GPT response\n",
    "    patterns = [\n",
    "        r'```python(.*?)```',\n",
    "        r'```(.*?)```',\n",
    "        r'\"\"\"(.*?)\"\"\"',\n",
    "        r'\"\"(.*?)\"\"',\n",
    "        r'\"(.*?)\"',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        code_string = re.search(pattern, response_cur, re.DOTALL)\n",
    "        if code_string is not None:\n",
    "            code_string = code_string.group(1).strip()\n",
    "            break\n",
    "    code_string = response_cur if not code_string else code_string\n",
    "    \n",
    "    # Remove unnecessary imports\n",
    "    lines = code_string.split(\"\\n\")\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().startswith(\"def \"):\n",
    "            code_string = \"\\n\".join(lines[i:])\n",
    "    try:\n",
    "        print(code_string)\n",
    "        gpt_reward_signature, input_lst = get_function_signature(code_string)\n",
    "        # print(\"a7eh\")\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Iteration {iter}: Code Run  cannot parse function signature!\")\n",
    "        # print(\"a7eh a7eh\")\n",
    "        continue\n",
    "    \n",
    "    code_runs.append(code_string)\n",
    "    reward_signature = [\n",
    "        f\"self.rew_buf[:], self.rew_dict = {gpt_reward_signature}\",\n",
    "        f\"self.extras['gpt_reward'] = self.rew_buf.mean()\",\n",
    "        f\"for rew_state in self.rew_dict: self.extras[rew_state] = self.rew_dict[rew_state].mean()\",\n",
    "    ]\n",
    "    indent = \" \" * 8\n",
    "    reward_signature = \"\\n\".join([indent + line for line in reward_signature])\n",
    "    if \"def compute_reward(self)\" in task_code_string:\n",
    "        task_code_string_iter = task_code_string.replace(\"def compute_reward(self):\", \"def compute_reward(self):\\n\" + reward_signature)\n",
    "    elif \"def compute_reward(self, actions)\" in task_code_string:\n",
    "        task_code_string_iter = task_code_string.replace(\"def compute_reward(self, actions):\", \"def compute_reward(self, actions):\\n\" + reward_signature)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    # Save the new environment code when the output contains valid code string!\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.writelines(task_code_string_iter + '\\n')\n",
    "        file.writelines(\"from typing import Tuple, Dict\" + '\\n')\n",
    "        file.writelines(\"import math\" + '\\n')\n",
    "        file.writelines(\"import torch\" + '\\n')\n",
    "        file.writelines(\"from torch import Tensor\" + '\\n')\n",
    "        if \"@torch.jit.script\" not in code_string:\n",
    "            code_string = \"@torch.jit.script\\n\" + code_string\n",
    "        file.writelines(code_string + '\\n')\n",
    "\n",
    "    with open(f\"env_iter{iter}_response{itt}_rewardonly.py\", 'w') as file:\n",
    "        file.writelines(code_string + '\\n')\n",
    "    # Copy the generated environment code to hydra output directory for bookkeeping\n",
    "    shutil.copy(output_file, f\"env_iter{iter}_response{itt}.py\")\n",
    "\n",
    "    # Find the freest GPU to run GPU-accelerated RL\n",
    "    # set_freest_gpu()\n",
    "\n",
    "    # Execute the python file with flags\n",
    "    rl_filepath = f\"env_iter{iter}_response{itt}.txt\"\n",
    "    \n",
    "    with open(rl_filepath, 'w') as f:\n",
    "        process = subprocess.Popen(['python', '-u', f'{ISAAC_ROOT_DIR}/train.py',  \n",
    "                                    'hydra/output=subprocess',\n",
    "                                    f'task={task}{suffix}', f'wandb_activate={cfg.use_wandb}',\n",
    "                                    f'wandb_entity={cfg.wandb_username}', f'wandb_project={cfg.wandb_project}',\n",
    "                                    f'headless={not cfg.capture_video}', f'capture_video={cfg.capture_video}', 'force_render=False',\n",
    "                                    f'max_iterations={cfg.max_iterations}'],\n",
    "                                    stdout=f, stderr=f)\n",
    "    block_until_training(rl_filepath, log_status=True, iter_num=iter, response_id=itt)\n",
    "    rl_runs.append(process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a49af2ff-8f0f-4f1f-89ee-00c7cf1fe959",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T13:05:14.565621906Z",
     "start_time": "2024-03-07T13:05:14.516284520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cartpole\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dafe87f2-313e-43a4-a5af-602aa117cd0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T17:54:22.105757552Z",
     "start_time": "2024-03-06T17:54:22.070148914Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cfe59ab-1812-49c1-88b0-590d4c4fcf47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T17:54:23.059026773Z",
     "start_time": "2024-03-06T17:54:22.070380995Z"
    }
   },
   "outputs": [],
   "source": [
    "# code_feedbacks = []\n",
    "# contents = []\n",
    "# successes = []\n",
    "# reward_correlations = []\n",
    "# code_paths = []\n",
    "# \n",
    "# exec_success = False \n",
    "# \n",
    "# for response_id, (code_run, rl_run) in enumerate(zip(code_runs, rl_runs)):\n",
    "#     rl_run.communicate()\n",
    "#     rl_filepath = f\"env_iter{iter}_response{response_id}.txt\"\n",
    "#     code_paths.append(f\"env_iter{iter}_response{response_id}.py\")\n",
    "#     try:\n",
    "#         with open(rl_filepath, 'r') as f:\n",
    "#             stdout_str = f.read() \n",
    "#     except: \n",
    "#         content = execution_error_feedback.format(traceback_msg=\"Code Run cannot be executed due to function signature error! Please re-write an entirely new reward function!\")\n",
    "#         content += code_output_tip\n",
    "#         contents.append(content) \n",
    "#         successes.append(DUMMY_FAILURE)\n",
    "#         reward_correlations.append(DUMMY_FAILURE)\n",
    "#         continue\n",
    "# \n",
    "#     content = ''\n",
    "#     traceback_msg = filter_traceback(stdout_str)\n",
    "    # if traceback_msg == '':\n",
    "    #     # If RL execution has no error, provide policy statistics feedback\n",
    "    #     exec_success = True\n",
    "    #     lines = stdout_str.split('\\n')\n",
    "    #     for i, line in enumerate(lines):\n",
    "    #         if line.startswith('Tensorboard Directory:'):\n",
    "    #             break \n",
    "    #     tensorboard_logdir = line.split(':')[-1].strip() \n",
    "    #     tensorboard_logs = load_tensorboard_logs(tensorboard_logdir)\n",
    "    #     max_iterations = np.array(tensorboard_logs['gt_reward']).shape[0]\n",
    "    #     epoch_freq = max(int(max_iterations // 10), 1)\n",
    "        \n",
    "    #     content += policy_feedback.format(epoch_freq=epoch_freq)\n",
    "        \n",
    "    #     # Compute Correlation between Human-Engineered and GPT Rewards\n",
    "    #     if \"gt_reward\" in tensorboard_logs and \"gpt_reward\" in tensorboard_logs:\n",
    "    #         gt_reward = np.array(tensorboard_logs[\"gt_reward\"])\n",
    "    #         gpt_reward = np.array(tensorboard_logs[\"gpt_reward\"])\n",
    "    #         reward_correlation = np.corrcoef(gt_reward, gpt_reward)[0, 1]\n",
    "    #         reward_correlations.append(reward_correlation)\n",
    "\n",
    "    #     # Add reward components log to the feedback\n",
    "    #     for metric in tensorboard_logs:\n",
    "    #         if \"/\" not in metric:\n",
    "    #             metric_cur = ['{:.2f}'.format(x) for x in tensorboard_logs[metric][::epoch_freq]]\n",
    "    #             metric_cur_max = max(tensorboard_logs[metric])\n",
    "    #             metric_cur_mean = sum(tensorboard_logs[metric]) / len(tensorboard_logs[metric])\n",
    "    #             if \"consecutive_successes\" == metric:\n",
    "    #                 successes.append(metric_cur_max)\n",
    "    #             metric_cur_min = min(tensorboard_logs[metric])\n",
    "    #             if metric != \"gt_reward\" and metric != \"gpt_reward\":\n",
    "    #                 if metric != \"consecutive_successes\":\n",
    "    #                     metric_name = metric \n",
    "    #                 else:\n",
    "    #                     metric_name = \"task_score\"\n",
    "    #                 content += f\"{metric_name}: {metric_cur}, Max: {metric_cur_max:.2f}, Mean: {metric_cur_mean:.2f}, Min: {metric_cur_min:.2f} \\n\"                    \n",
    "    #             else:\n",
    "    #                 # Provide ground-truth score when success rate not applicable\n",
    "    #                 if \"consecutive_successes\" not in tensorboard_logs:\n",
    "    #                     content += f\"ground-truth score: {metric_cur}, Max: {metric_cur_max:.2f}, Mean: {metric_cur_mean:.2f}, Min: {metric_cur_min:.2f} \\n\"                    \n",
    "    #     code_feedbacks.append(code_feedback)\n",
    "    #     content += code_feedback  \n",
    "    # else:\n",
    "    #     # Otherwise, provide execution traceback error feedback\n",
    "    #     successes.append(DUMMY_FAILURE)\n",
    "    #     reward_correlations.append(DUMMY_FAILURE)\n",
    "    #     content += execution_error_feedback.format(traceback_msg=traceback_msg)\n",
    "\n",
    "    # content += code_output_tip\n",
    "    # contents.append(content) \n",
    "\n",
    "# Repeat the iteration if all code generation failed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bbec8fb-0f4c-48c4-b206-a8bc7a37f7f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T17:54:23.114155400Z",
     "start_time": "2024-03-06T17:54:23.040005157Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 322)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m<tokenize>:322\u001B[0;36m\u001B[0m\n\u001B[0;31m    if max_reward_code_path is None:\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# @hydra.main(config_path=\"cfg\", config_name=\"config\", version_base=\"1.1\")\n",
    "# def main(cfg):\n",
    "#     workspace_dir = Path.cwd()\n",
    "#     logging.info(f\"Workspace: {workspace_dir}\")\n",
    "#     logging.info(f\"Project Root: {EUREKA_ROOT_DIR}\")\n",
    "\n",
    "    # openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    # task = cfg.env.task\n",
    "    # task_description = cfg.env.description\n",
    "    # suffix = cfg.suffix\n",
    "    # model = cfg.model\n",
    "    # logging.info(f\"Using LLM: {model}\")\n",
    "    # logging.info(\"Task: \" + task)\n",
    "    # logging.info(\"Task description: \" + task_description)\n",
    "\n",
    "    # env_name = cfg.env.env_name.lower()\n",
    "    # env_parent = 'isaac' if f'{env_name}.py' in os.listdir(f'{EUREKA_ROOT_DIR}/envs/isaac') else 'dexterity'\n",
    "    # task_file = f'{EUREKA_ROOT_DIR}/envs/{env_parent}/{env_name}.py'\n",
    "    # task_obs_file = f'{EUREKA_ROOT_DIR}/envs/{env_parent}/{env_name}_obs.py'\n",
    "    # shutil.copy(task_obs_file, f\"env_init_obs.py\")\n",
    "    # task_code_string  = file_to_string(task_file)\n",
    "    # task_obs_code_string  = file_to_string(task_obs_file)\n",
    "    # output_file = f\"{ISAAC_ROOT_DIR}/tasks/{env_name}{suffix.lower()}.py\"\n",
    "\n",
    "    # # Loading all text prompts\n",
    "    # prompt_dir = f'{EUREKA_ROOT_DIR}/utils/prompts'\n",
    "    # initial_system = file_to_string(f'{prompt_dir}/initial_system.txt')\n",
    "    # code_output_tip = file_to_string(f'{prompt_dir}/code_output_tip.txt')\n",
    "    # code_feedback = file_to_string(f'{prompt_dir}/code_feedback.txt')\n",
    "    # initial_user = file_to_string(f'{prompt_dir}/initial_user.txt')\n",
    "    # reward_signature = file_to_string(f'{prompt_dir}/reward_signature.txt')\n",
    "    # policy_feedback = file_to_string(f'{prompt_dir}/policy_feedback.txt')\n",
    "    # execution_error_feedback = file_to_string(f'{prompt_dir}/execution_error_feedback.txt')\n",
    "\n",
    "    # initial_system = initial_system.format(task_reward_signature_string=reward_signature) + code_output_tip\n",
    "    # initial_user = initial_user.format(task_obs_code_string=task_obs_code_string, task_description=task_description)\n",
    "    # messages = [{\"role\": \"system\", \"content\": initial_system}, {\"role\": \"user\", \"content\": initial_user}]\n",
    "\n",
    "    # task_code_string = task_code_string.replace(task, task+suffix)\n",
    "    # # Create Task YAML files\n",
    "    # create_task(ISAAC_ROOT_DIR, cfg.env.task, cfg.env.env_name, suffix)\n",
    "\n",
    "    # DUMMY_FAILURE = -10000.\n",
    "    # max_successes = []\n",
    "    # max_successes_reward_correlation = []\n",
    "    # execute_rates = []\n",
    "    # best_code_paths = []\n",
    "    # max_success_overall = DUMMY_FAILURE\n",
    "    # max_success_reward_correlation_overall = DUMMY_FAILURE\n",
    "    # max_reward_code_path = None \n",
    "    \n",
    "    # # Eureka generation loop\n",
    "    # for iter in range(cfg.iteration):\n",
    "    #     # Get Eureka response\n",
    "    #     responses = []\n",
    "    #     response_cur = None\n",
    "    #     total_samples = 0\n",
    "    #     total_token = 0\n",
    "    #     total_completion_token = 0\n",
    "    #     chunk_size = cfg.sample if \"gpt-3.5\" in model else 4\n",
    "\n",
    "    #     logging.info(f\"Iteration {iter}: Generating {cfg.sample} samples with {cfg.model}\")\n",
    "\n",
    "    #     while True:\n",
    "    #         if total_samples >= cfg.sample:\n",
    "    #             break\n",
    "    #         for attempt in range(1000):\n",
    "    #             try:\n",
    "    #                 response_cur = openai.ChatCompletion.create(\n",
    "    #                     model=model,\n",
    "    #                     messages=messages,\n",
    "    #                     temperature=cfg.temperature,\n",
    "    #                     n=chunk_size\n",
    "    #                 )\n",
    "    #                 total_samples += chunk_size\n",
    "    #                 break\n",
    "    #             except Exception as e:\n",
    "    #                 if attempt >= 10:\n",
    "    #                     chunk_size = max(int(chunk_size / 2), 1)\n",
    "    #                     print(\"Current Chunk Size\", chunk_size)\n",
    "    #                 logging.info(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "    #                 time.sleep(1)\n",
    "    #         if response_cur is None:\n",
    "    #             logging.info(\"Code terminated due to too many failed attempts!\")\n",
    "    #             exit()\n",
    "\n",
    "    #         responses.extend(response_cur[\"choices\"])\n",
    "    #         prompt_tokens = response_cur[\"usage\"][\"prompt_tokens\"]\n",
    "    #         total_completion_token += response_cur[\"usage\"][\"completion_tokens\"]\n",
    "    #         total_token += response_cur[\"usage\"][\"total_tokens\"]\n",
    "\n",
    "    #     if cfg.sample == 1:\n",
    "    #         logging.info(f\"Iteration {iter}: GPT Output:\\n \" + responses[0][\"message\"][\"content\"] + \"\\n\")\n",
    "\n",
    "    #     # Logging Token Information\n",
    "    #     logging.info(f\"Iteration {iter}: Prompt Tokens: {prompt_tokens}, Completion Tokens: {total_completion_token}, Total Tokens: {total_token}\")\n",
    "        \n",
    "    #     code_runs = [] \n",
    "    #     rl_runs = []\n",
    "    #     for response_id in range(cfg.sample):\n",
    "    #         response_cur = responses[response_id][\"message\"][\"content\"]\n",
    "    #         logging.info(f\"Iteration {iter}: Processing Code Run {response_id}\")\n",
    "\n",
    "    #         # Regex patterns to extract python code enclosed in GPT response\n",
    "    #         patterns = [\n",
    "    #             r'```python(.*?)```',\n",
    "    #             r'```(.*?)```',\n",
    "    #             r'\"\"\"(.*?)\"\"\"',\n",
    "    #             r'\"\"(.*?)\"\"',\n",
    "    #             r'\"(.*?)\"',\n",
    "    #         ]\n",
    "    #         for pattern in patterns:\n",
    "    #             code_string = re.search(pattern, response_cur, re.DOTALL)\n",
    "    #             if code_string is not None:\n",
    "    #                 code_string = code_string.group(1).strip()\n",
    "    #                 break\n",
    "    #         code_string = response_cur if not code_string else code_string\n",
    "\n",
    "    #         # Remove unnecessary imports\n",
    "    #         lines = code_string.split(\"\\n\")\n",
    "    #         for i, line in enumerate(lines):\n",
    "    #             if line.strip().startswith(\"def \"):\n",
    "    #                 code_string = \"\\n\".join(lines[i:])\n",
    "                    \n",
    "    #         # Add the Eureka Reward Signature to the environment code\n",
    "    #         try:\n",
    "    #             gpt_reward_signature, input_lst = get_function_signature(code_string)\n",
    "    #         except Exception as e:\n",
    "    #             logging.info(f\"Iteration {iter}: Code Run {response_id} cannot parse function signature!\")\n",
    "    #             continue\n",
    "\n",
    "    #         code_runs.append(code_string)\n",
    "    #         reward_signature = [\n",
    "    #             f\"self.rew_buf[:], self.rew_dict = {gpt_reward_signature}\",\n",
    "    #             f\"self.extras['gpt_reward'] = self.rew_buf.mean()\",\n",
    "    #             f\"for rew_state in self.rew_dict: self.extras[rew_state] = self.rew_dict[rew_state].mean()\",\n",
    "    #         ]\n",
    "    #         indent = \" \" * 8\n",
    "    #         reward_signature = \"\\n\".join([indent + line for line in reward_signature])\n",
    "    #         if \"def compute_reward(self)\" in task_code_string:\n",
    "    #             task_code_string_iter = task_code_string.replace(\"def compute_reward(self):\", \"def compute_reward(self):\\n\" + reward_signature)\n",
    "    #         elif \"def compute_reward(self, actions)\" in task_code_string:\n",
    "    #             task_code_string_iter = task_code_string.replace(\"def compute_reward(self, actions):\", \"def compute_reward(self, actions):\\n\" + reward_signature)\n",
    "    #         else:\n",
    "    #             raise NotImplementedError\n",
    "\n",
    "            # Save the new environment code when the output contains valid code string!\n",
    "            # with open(output_file, 'w') as file:\n",
    "            #     file.writelines(task_code_string_iter + '\\n')\n",
    "            #     file.writelines(\"from typing import Tuple, Dict\" + '\\n')\n",
    "            #     file.writelines(\"import math\" + '\\n')\n",
    "            #     file.writelines(\"import torch\" + '\\n')\n",
    "            #     file.writelines(\"from torch import Tensor\" + '\\n')\n",
    "            #     if \"@torch.jit.script\" not in code_string:\n",
    "            #         code_string = \"@torch.jit.script\\n\" + code_string\n",
    "            #     file.writelines(code_string + '\\n')\n",
    "\n",
    "            # with open(f\"env_iter{iter}_response{response_id}_rewardonly.py\", 'w') as file:\n",
    "            #     file.writelines(code_string + '\\n')\n",
    "\n",
    "            # Copy the generated environment code to hydra output directory for bookkeeping\n",
    "            # shutil.copy(output_file, f\"env_iter{iter}_response{response_id}.py\")\n",
    "\n",
    "            # # Find the freest GPU to run GPU-accelerated RL\n",
    "            # set_freest_gpu()\n",
    "            \n",
    "            # # Execute the python file with flags\n",
    "            # rl_filepath = f\"env_iter{iter}_response{response_id}.txt\"\n",
    "            # with open(rl_filepath, 'w') as f:\n",
    "            #     process = subprocess.Popen(['python', '-u', f'{ISAAC_ROOT_DIR}/train.py',  \n",
    "            #                                 'hydra/output=subprocess',\n",
    "            #                                 f'task={task}{suffix}', f'wandb_activate={cfg.use_wandb}',\n",
    "            #                                 f'wandb_entity={cfg.wandb_username}', f'wandb_project={cfg.wandb_project}',\n",
    "            #                                 f'headless={not cfg.capture_video}', f'capture_video={cfg.capture_video}', 'force_render=False',\n",
    "            #                                 f'max_iterations={cfg.max_iterations}'],\n",
    "            #                                 stdout=f, stderr=f)\n",
    "            # block_until_training(rl_filepath, log_status=True, iter_num=iter, response_id=response_id)\n",
    "            # rl_runs.append(process)\n",
    "        \n",
    "        # Gather RL training results and construct reward reflection\n",
    "        # code_feedbacks = []\n",
    "        # contents = []\n",
    "        # successes = []\n",
    "        # reward_correlations = []\n",
    "        # code_paths = []\n",
    "        \n",
    "        # exec_success = False \n",
    "        for response_id, (code_run, rl_run) in enumerate(zip(code_runs, rl_runs)):\n",
    "            rl_run.communicate()\n",
    "            rl_filepath = f\"env_iter{iter}_response{response_id}.txt\"\n",
    "            code_paths.append(f\"env_iter{iter}_response{response_id}.py\")\n",
    "            try:\n",
    "                with open(rl_filepath, 'r') as f:\n",
    "                    stdout_str = f.read() \n",
    "            except: \n",
    "                content = execution_error_feedback.format(traceback_msg=\"Code Run cannot be executed due to function signature error! Please re-write an entirely new reward function!\")\n",
    "                content += code_output_tip\n",
    "                contents.append(content) \n",
    "                successes.append(DUMMY_FAILURE)\n",
    "                reward_correlations.append(DUMMY_FAILURE)\n",
    "                continue\n",
    "\n",
    "            content = ''\n",
    "            traceback_msg = filter_traceback(stdout_str)\n",
    "\n",
    "            if traceback_msg == '':\n",
    "                # If RL execution has no error, provide policy statistics feedback\n",
    "                exec_success = True\n",
    "                lines = stdout_str.split('\\n')\n",
    "                for i, line in enumerate(lines):\n",
    "                    if line.startswith('Tensorboard Directory:'):\n",
    "                        break \n",
    "                tensorboard_logdir = line.split(':')[-1].strip() \n",
    "                tensorboard_logs = load_tensorboard_logs(tensorboard_logdir)\n",
    "                max_iterations = np.array(tensorboard_logs['gt_reward']).shape[0]\n",
    "                epoch_freq = max(int(max_iterations // 10), 1)\n",
    "                \n",
    "                content += policy_feedback.format(epoch_freq=epoch_freq)\n",
    "                \n",
    "                # Compute Correlation between Human-Engineered and GPT Rewards\n",
    "                if \"gt_reward\" in tensorboard_logs and \"gpt_reward\" in tensorboard_logs:\n",
    "                    gt_reward = np.array(tensorboard_logs[\"gt_reward\"])\n",
    "                    gpt_reward = np.array(tensorboard_logs[\"gpt_reward\"])\n",
    "                    reward_correlation = np.corrcoef(gt_reward, gpt_reward)[0, 1]\n",
    "                    reward_correlations.append(reward_correlation)\n",
    "\n",
    "                # Add reward components log to the feedback\n",
    "                for metric in tensorboard_logs:\n",
    "                    if \"/\" not in metric:\n",
    "                        metric_cur = ['{:.2f}'.format(x) for x in tensorboard_logs[metric][::epoch_freq]]\n",
    "                        metric_cur_max = max(tensorboard_logs[metric])\n",
    "                        metric_cur_mean = sum(tensorboard_logs[metric]) / len(tensorboard_logs[metric])\n",
    "                        if \"consecutive_successes\" == metric:\n",
    "                            successes.append(metric_cur_max)\n",
    "                        metric_cur_min = min(tensorboard_logs[metric])\n",
    "                        if metric != \"gt_reward\" and metric != \"gpt_reward\":\n",
    "                            if metric != \"consecutive_successes\":\n",
    "                                metric_name = metric \n",
    "                            else:\n",
    "                                metric_name = \"task_score\"\n",
    "                            content += f\"{metric_name}: {metric_cur}, Max: {metric_cur_max:.2f}, Mean: {metric_cur_mean:.2f}, Min: {metric_cur_min:.2f} \\n\"                    \n",
    "                        else:\n",
    "                            # Provide ground-truth score when success rate not applicable\n",
    "                            if \"consecutive_successes\" not in tensorboard_logs:\n",
    "                                content += f\"ground-truth score: {metric_cur}, Max: {metric_cur_max:.2f}, Mean: {metric_cur_mean:.2f}, Min: {metric_cur_min:.2f} \\n\"                    \n",
    "                code_feedbacks.append(code_feedback)\n",
    "                content += code_feedback  \n",
    "            else:\n",
    "                # Otherwise, provide execution traceback error feedback\n",
    "                successes.append(DUMMY_FAILURE)\n",
    "                reward_correlations.append(DUMMY_FAILURE)\n",
    "                content += execution_error_feedback.format(traceback_msg=traceback_msg)\n",
    "\n",
    "            content += code_output_tip\n",
    "            contents.append(content) \n",
    "        \n",
    "        # Repeat the iteration if all code generation failed\n",
    "        if not exec_success and cfg.sample != 1:\n",
    "            execute_rates.append(0.)\n",
    "            max_successes.append(DUMMY_FAILURE)\n",
    "            max_successes_reward_correlation.append(DUMMY_FAILURE)\n",
    "            best_code_paths.append(None)\n",
    "            logging.info(\"All code generation failed! Repeat this iteration from the current message checkpoint!\")\n",
    "            continue\n",
    "\n",
    "        # Select the best code sample based on the success rate\n",
    "        best_sample_idx = np.argmax(np.array(successes))\n",
    "        best_content = contents[best_sample_idx]\n",
    "            \n",
    "        max_success = successes[best_sample_idx]\n",
    "        max_success_reward_correlation = reward_correlations[best_sample_idx]\n",
    "        execute_rate = np.sum(np.array(successes) >= 0.) / cfg.sample\n",
    "\n",
    "        # Update the best Eureka Output\n",
    "        if max_success > max_success_overall:\n",
    "            max_success_overall = max_success\n",
    "            max_success_reward_correlation_overall = max_success_reward_correlation\n",
    "            max_reward_code_path = code_paths[best_sample_idx]\n",
    "\n",
    "        execute_rates.append(execute_rate)\n",
    "        max_successes.append(max_success)\n",
    "        max_successes_reward_correlation.append(max_success_reward_correlation)\n",
    "        best_code_paths.append(code_paths[best_sample_idx])\n",
    "\n",
    "        logging.info(f\"Iteration {iter}: Max Success: {max_success}, Execute Rate: {execute_rate}, Max Success Reward Correlation: {max_success_reward_correlation}\")\n",
    "        logging.info(f\"Iteration {iter}: Best Generation ID: {best_sample_idx}\")\n",
    "        logging.info(f\"Iteration {iter}: GPT Output Content:\\n\" +  responses[best_sample_idx][\"message\"][\"content\"] + \"\\n\")\n",
    "        logging.info(f\"Iteration {iter}: User Content:\\n\" + best_content + \"\\n\")\n",
    "            \n",
    "        # Plot the success rate\n",
    "        fig, axs = plt.subplots(2, figsize=(6, 6))\n",
    "        fig.suptitle(f'{cfg.env.task}')\n",
    "\n",
    "        x_axis = np.arange(len(max_successes))\n",
    "\n",
    "        axs[0].plot(x_axis, np.array(max_successes))\n",
    "        axs[0].set_title(\"Max Success\")\n",
    "        axs[0].set_xlabel(\"Iteration\")\n",
    "\n",
    "        axs[1].plot(x_axis, np.array(execute_rates))\n",
    "        axs[1].set_title(\"Execute Rate\")\n",
    "        axs[1].set_xlabel(\"Iteration\")\n",
    "\n",
    "        fig.tight_layout(pad=3.0)\n",
    "        plt.savefig('summary.png')\n",
    "        np.savez('summary.npz', max_successes=max_successes, execute_rates=execute_rates, best_code_paths=best_code_paths, max_successes_reward_correlation=max_successes_reward_correlation)\n",
    "\n",
    "        if len(messages) == 2:\n",
    "            messages += [{\"role\": \"assistant\", \"content\": responses[best_sample_idx][\"message\"][\"content\"]}]\n",
    "            messages += [{\"role\": \"user\", \"content\": best_content}]\n",
    "        else:\n",
    "            assert len(messages) == 4\n",
    "            messages[-2] = {\"role\": \"assistant\", \"content\": responses[best_sample_idx][\"message\"][\"content\"]}\n",
    "            messages[-1] = {\"role\": \"user\", \"content\": best_content}\n",
    "\n",
    "        # Save dictionary as JSON file\n",
    "        with open('messages.json', 'w') as file:\n",
    "            json.dump(messages, file, indent=4)\n",
    "    \n",
    "    # Evaluate the best reward code many times\n",
    "    if max_reward_code_path is None: \n",
    "        logging.info(\"All iterations of code generation failed, aborting...\")\n",
    "        logging.info(\"Please double check the output env_iter*_response*.txt files for repeating errors!\")\n",
    "        exit()\n",
    "    logging.info(f\"Task: {task}, Max Training Success {max_success_overall}, Correlation {max_success_reward_correlation_overall}, Best Reward Code Path: {max_reward_code_path}\")\n",
    "    logging.info(f\"Evaluating best reward code {cfg.num_eval} times\")\n",
    "    shutil.copy(max_reward_code_path, output_file)\n",
    "    \n",
    "    eval_runs = []\n",
    "    for i in range(cfg.num_eval):\n",
    "        set_freest_gpu()\n",
    "        \n",
    "        # Execute the python file with flags\n",
    "        rl_filepath = f\"reward_code_eval{i}.txt\"\n",
    "        with open(rl_filepath, 'w') as f:\n",
    "            process = subprocess.Popen(['python', '-u', f'{ISAAC_ROOT_DIR}/train.py',  \n",
    "                                        'hydra/output=subprocess',\n",
    "                                        f'task={task}{suffix}', f'wandb_activate={cfg.use_wandb}',\n",
    "                                        f'wandb_entity={cfg.wandb_username}', f'wandb_project={cfg.wandb_project}',\n",
    "                                        f'headless={not cfg.capture_video}', f'capture_video={cfg.capture_video}', 'force_render=False', f'seed={i}',\n",
    "                                        ],\n",
    "                                        stdout=f, stderr=f)\n",
    "\n",
    "        block_until_training(rl_filepath)\n",
    "        eval_runs.append(process)\n",
    "\n",
    "    reward_code_final_successes = []\n",
    "    reward_code_correlations_final = []\n",
    "    for i, rl_run in enumerate(eval_runs):\n",
    "        rl_run.communicate()\n",
    "        rl_filepath = f\"reward_code_eval{i}.txt\"\n",
    "        with open(rl_filepath, 'r') as f:\n",
    "            stdout_str = f.read() \n",
    "        lines = stdout_str.split('\\n')\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.startswith('Tensorboard Directory:'):\n",
    "                break \n",
    "        tensorboard_logdir = line.split(':')[-1].strip() \n",
    "        tensorboard_logs = load_tensorboard_logs(tensorboard_logdir)\n",
    "        max_success = max(tensorboard_logs['consecutive_successes'])\n",
    "        reward_code_final_successes.append(max_success)\n",
    "\n",
    "        if \"gt_reward\" in tensorboard_logs and \"gpt_reward\" in tensorboard_logs:\n",
    "            gt_reward = np.array(tensorboard_logs[\"gt_reward\"])\n",
    "            gpt_reward = np.array(tensorboard_logs[\"gpt_reward\"])\n",
    "            reward_correlation = np.corrcoef(gt_reward, gpt_reward)[0, 1]\n",
    "            reward_code_correlations_final.append(reward_correlation)\n",
    "\n",
    "    logging.info(f\"Final Success Mean: {np.mean(reward_code_final_successes)}, Std: {np.std(reward_code_final_successes)}, Raw: {reward_code_final_successes}\")\n",
    "    logging.info(f\"Final Correlation Mean: {np.mean(reward_code_correlations_final)}, Std: {np.std(reward_code_correlations_final)}, Raw: {reward_code_correlations_final}\")\n",
    "    np.savez('final_eval.npz', reward_code_final_successes=reward_code_final_successes, reward_code_correlations_final=reward_code_correlations_final)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f3181-e6c3-4183-961e-d2c140de2d8b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-06T17:54:23.086386172Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
